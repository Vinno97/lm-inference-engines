# Open Inference Engines

Feel free to create a PR or issue if you want a new engine column, feature row, or update a status. 

### Compared Inference Engines

- TGI: https://github.com/huggingface/text-generation-inference/
- vLLM: https://github.com/vllm-project/vllm/
- llama.cpp: https://github.com/ggerganov/llama.cpp/
- TensorRT-LLM: https://github.com/NVIDIA/TensorRT-LLM

### Comparison Table

‚úÖ Included | ‚òëÔ∏è Similar Feature | üî® In progress / PR |üóìÔ∏è Planned / on Roadmap |‚ùì Unclear / Not official |‚ùå Not supported



|                          | vLLM                  | TensorRT-LLM              | llama.cpp                                   | TGI               | LightLLM           |
|--------------------------|-----------------------|---------------------------|---------------------------------------------|-------------------|--------------------|
| **Performance**          |                       |                           |                                             |                   |                    |
| FlashAttention           | ‚òëÔ∏è (xFormers) [^4]    | ‚úÖ [^16]                   | ‚ùì                                           | ‚úÖ [^1]            | ‚úÖ                  |
| PagedAttention           | ‚úÖ [^1]               | ‚úÖ [^16]                   | ‚ùå [^10]                                    | ‚úÖ                 | ‚òëÔ∏è (TokenAttention) [^19] |
| Speculative Decoding     | üî® [^8]               | üóìÔ∏è [^2]                   | ‚úÖ [^11]                                    | üî® [^3]            | ‚ùå                  |
| Tensor Parallel          | ‚úÖ                    | ‚úÖ [^17]                   | ‚òëÔ∏è ** [^12]          | ‚úÖ [^5]            | ‚úÖ                  |
| Pipeline Parallel        | ‚úÖ                    | ‚úÖ [^17]                   | ‚úÖ                                          | ‚ùì [^5]            | ‚ùå                  |
| **Functionality**        |                       |                           |                                             |                   |                    |
| OpenAI Compatible API    | ‚úÖ                    |                           | ‚úÖ [^13]                                    | ‚ùì                 | ‚úÖ [^20]            |
| Grammars                 | ‚ùå [^9]               | ‚ùå                         | ‚úÖ [^13]                                    | ‚ùå [^6]            | ‚ùå                  |
| Beam Search              | ‚úÖ                    | ‚úÖ [^16]                   | ‚úÖ [^14]                                    | ‚ùå [^7]            | ‚ùå                  |
| **Quantization**         |                       |                           |                                             |                   |                    |
| AWQ                      | ‚úÖ                    | ‚úÖ                         | ‚ùå                                          | ‚úÖ                 | ‚ùå                  |
| Other Quants             | SqueezeLLM            | ‚ùå                         | GGUF                                        | GPTQ, BnB, EEQT [^18]| ‚ùì              |
| **Models**               |                       |                           |                                             |                   |                    |
| LlamaForCausalLM         | ‚úÖ                    | ‚úÖ                         | ‚úÖ                                          | ‚úÖ                 | ‚úÖ                  |
| MistralForCausalLM       | ‚úÖ                    | ‚úÖ                         | ‚úÖ                                          | ‚úÖ                 | üóìÔ∏è [^21]          |
| **Implementation**       |                       |                           |                                             |                   |                    |
| Core Language            | Python                | C++                       | C++                                        | Python / Rust     | Python             |
| GPU Kernel Language      | CUDA *                 | CUDA *                    | CUDA                                       | CUDA *            | **Triton** / CUDA  |
| **Repo**                 |                       |                           |                                             |                   |                    |
| License                  | Apache 2.0            | Apache 2.0                | MIT                                        | HFOILv1.0 [^15]   | Apache 2.0         |
| Github Stars             | 11K                   | 4K                        | 46K                                         | 6K                | 1K                 |

*Supports Triton for one-off such as FlashAttention (FusedAttention) / or AWQ, or allows Triton plugins but doesn't use Triton otherwise.
**Sequentially processed tensor split

[^1]: https://github.com/huggingface/text-generation-inference/issues/753#issuecomment-1663525606
[^2]: https://github.com/NVIDIA/TensorRT-LLM/issues/169
[^3]: https://github.com/huggingface/text-generation-inference/pull/1308
[^4]: https://github.com/vllm-project/vllm/pull/70
[^5]: https://github.com/huggingface/text-generation-inference/issues/1031#issuecomment-1727976990
[^6]: https://github.com/huggingface/text-generation-inference/issues/1050
[^7]: https://github.com/huggingface/text-generation-inference/issues/722#issuecomment-1658823644
[^8]: https://github.com/vllm-project/vllm/pull/1797
[^9]: https://github.com/vllm-project/vllm/issues/1229
[^10]: https://github.com/ggerganov/llama.cpp/issues/1955
[^11]: https://github.com/ggerganov/llama.cpp/blob/fe680e3d1080a765e5d3150ffd7bab189742898d/examples/speculative/README.md
[^12]: https://github.com/ggerganov/llama.cpp/issues/4014#issuecomment-1804925896
[^13]: https://github.com/ggerganov/llama.cpp/tree/master/examples/server
[^14]: https://github.com/ggerganov/llama.cpp/tree/master/examples/beam-search
[^15]: https://raw.githubusercontent.com/huggingface/text-generation-inference/main/LICENSE
[^16]: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/gpt_attention.md
[^17]: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/pybind/bindings.cpp#L184
[^18]: https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/cli.py#L15-L21
[^19]: https://github.com/ModelTC/lightllm/blob/main/docs/TokenAttention.md
[^20]: https://github.com/ModelTC/lightllm/blob/main/lightllm/server/api_models.py#L9
[^21]: https://github.com/ModelTC/lightllm/issues/224#issuecomment-1827365514

## Notes

FlexGen is going through a refactor, so it's mostly been left blank.
